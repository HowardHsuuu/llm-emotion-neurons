{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI5TyZec4ezp",
        "outputId": "ff430d92-8936-4117-f68e-22f81a1879e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llm-emotion-neurons'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 46 (delta 19), reused 36 (delta 12), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (46/46), 23.94 KiB | 7.98 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HowardHsuuu/llm-emotion-neurons.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmzt81Mj5EFe",
        "outputId": "6c52627a-7bfe-4360-c901-d6cfec869021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/llm-emotion-neurons\n"
          ]
        }
      ],
      "source": [
        "%cd llm-emotion-neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkhxhGJB6aca",
        "outputId": "fd85b57c-fc7e-4f28-877c-0c885850793a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting hf_xet\n",
            "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_xet\n",
            "Successfully installed hf_xet-1.1.0\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.51.3)\n",
            "Requirement already satisfied: datasets>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.15.2)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.67.1)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.11.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.11.0->-r requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.11.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.11.0->-r requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.11.0->-r requirements.txt (line 3)) (3.11.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->-r requirements.txt (line 5)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->-r requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->-r requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 8)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 8)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 8)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 8)) (3.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.11.0->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.11.0->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.11.0->-r requirements.txt (line 3)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.11.0->-r requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.11.0->-r requirements.txt (line 3)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.11.0->-r requirements.txt (line 3)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.11.0->-r requirements.txt (line 3)) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install hf_xet\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecGaK2Cm6TNv",
        "outputId": "0bfba43c-423d-4395-bc6e-7fb6b6c17e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf5dFtXx5r5m",
        "outputId": "1aad1bc9-f679-4360-bcf0-b0d4d8e35ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "README.md: 100% 9.40k/9.40k [00:00<00:00, 42.9MB/s]\n",
            "train-00000-of-00001.parquet: 100% 2.77M/2.77M [00:00<00:00, 67.4MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 350k/350k [00:00<00:00, 327MB/s]\n",
            "test-00000-of-00001.parquet: 100% 347k/347k [00:00<00:00, 269MB/s]\n",
            "Generating train split: 100% 43410/43410 [00:00<00:00, 696345.06 examples/s]\n",
            "Generating validation split: 100% 5426/5426 [00:00<00:00, 1086625.93 examples/s]\n",
            "Generating test split: 100% 5427/5427 [00:00<00:00, 1160463.31 examples/s]\n",
            "Filter: 100% 43410/43410 [00:00<00:00, 170224.55 examples/s]\n",
            "Filter: 100% 43410/43410 [00:00<00:00, 149727.34 examples/s]\n",
            "Saved 1025 'anger' samples and 12823 neutral samples.\n"
          ]
        }
      ],
      "source": [
        "!python src/data_loader_emo.py --emotion anger --split train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_7e74pv9tQS",
        "outputId": "97be629c-37a6-4c97-bd49-51fe00216b1c"
      },
      "outputs": [],
      "source": [
        "! huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRaKFFZP599P",
        "outputId": "385b1e55-2401-4244-b4c7-d7ad8de90755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1025 emotion and 12823 neutral texts\n",
            "tokenizer_config.json: 100% 54.5k/54.5k [00:00<00:00, 170MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:01<00:00, 6.72MB/s]\n",
            "special_tokens_map.json: 100% 296/296 [00:00<00:00, 2.14MB/s]\n",
            "config.json: 100% 878/878 [00:00<00:00, 5.79MB/s]\n",
            "2025-05-01 14:14:32.114839: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-01 14:14:32.135741: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746108872.156951    1568 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746108872.163108    1568 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-01 14:14:32.185269: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors.index.json: 100% 20.9k/20.9k [00:00<00:00, 75.7MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/1.46G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/4.97G [00:00<00:18, 270MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 21.0M/1.46G [00:00<00:10, 132MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 52.4M/1.46G [00:00<00:07, 196MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.97G [00:00<00:23, 210MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 105M/1.46G [00:00<00:04, 307MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 94.4M/4.97G [00:00<00:21, 223MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 157M/1.46G [00:00<00:03, 371MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.97G [00:00<00:20, 237MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 210M/1.46G [00:00<00:03, 391MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 178M/4.97G [00:00<00:17, 268MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 262M/1.46G [00:00<00:02, 421MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 220M/4.97G [00:00<00:15, 297MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 315M/1.46G [00:00<00:02, 405MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 357M/1.46G [00:00<00:02, 406MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.97G [00:01<00:19, 239MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 409M/1.46G [00:01<00:02, 408MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 283M/4.97G [00:01<00:19, 240MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 451M/1.46G [00:01<00:02, 409MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.97G [00:01<00:19, 237MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 503M/1.46G [00:01<00:02, 421MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 556M/1.46G [00:01<00:02, 424MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 346M/4.97G [00:01<00:20, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 377M/4.97G [00:01<00:18, 242MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 608M/1.46G [00:01<00:02, 404MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 409M/4.97G [00:01<00:18, 248MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 650M/1.46G [00:01<00:02, 391MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 451M/4.97G [00:01<00:16, 270MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 692M/1.46G [00:01<00:01, 388MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.97G [00:01<00:14, 306MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 734M/1.46G [00:01<00:01, 377MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.97G [00:02<00:13, 331MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 776M/1.46G [00:02<00:01, 375MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.97G [00:02<00:14, 306MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 828M/1.46G [00:02<00:01, 385MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 881M/1.46G [00:02<00:01, 304MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.97G [00:02<00:18, 231MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 923M/1.46G [00:02<00:01, 304MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 650M/4.97G [00:02<00:21, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.97G [00:02<00:20, 208MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 965M/1.46G [00:02<00:02, 239MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.97G [00:02<00:19, 220MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 996M/1.46G [00:03<00:02, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.03G/1.46G [00:03<00:01, 238MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.97G [00:03<00:21, 192MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.07G/1.46G [00:03<00:01, 233MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.97G [00:03<00:21, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.97G [00:03<00:30, 138MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.10G/1.46G [00:03<00:02, 170MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.97G [00:03<00:23, 179MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 1.14G/1.46G [00:03<00:01, 167MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.97G [00:03<00:23, 172MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 1.17G/1.46G [00:04<00:01, 184MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.97G [00:04<00:22, 180MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 1.21G/1.46G [00:04<00:01, 188MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 923M/4.97G [00:04<00:22, 183MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 1.25G/1.46G [00:04<00:00, 225MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 975M/4.97G [00:04<00:16, 243MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 1.28G/1.46G [00:04<00:00, 236MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.97G [00:04<00:18, 217MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 1.31G/1.46G [00:04<00:00, 220MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/4.97G [00:04<00:19, 197MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 1.34G/1.46G [00:04<00:00, 205MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 1.07G/4.97G [00:04<00:21, 181MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 1.37G/1.46G [00:04<00:00, 185MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 1.39G/1.46G [00:05<00:00, 172MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:05<00:25, 149MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 1.43G/1.46G [00:05<00:00, 191MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.97G [00:05<00:27, 139MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 1.46G/1.46G [00:05<00:00, 273MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.97G [00:05<00:25, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.97G [00:05<00:26, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.97G [00:05<00:22, 172MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/4.97G [00:05<00:20, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.97G [00:05<00:18, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 1.27G/4.97G [00:06<00:30, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.97G [00:06<00:25, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.97G [00:06<00:21, 167MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.97G [00:06<00:19, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/4.97G [00:06<00:17, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.97G [00:07<00:16, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.97G [00:07<00:16, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 1.52G/4.97G [00:07<00:13, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.97G [00:07<00:13, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.97G [00:07<00:12, 268MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.97G [00:07<00:16, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 1.66G/4.97G [00:08<00:18, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.97G [00:08<00:15, 215MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.97G [00:08<00:12, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.97G [00:08<00:12, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 1.81G/4.97G [00:08<00:10, 292MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.97G [00:08<00:11, 277MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.97G [00:08<00:11, 280MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 1.92G/4.97G [00:09<00:12, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.97G [00:09<00:13, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.97G [00:09<00:12, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 2.01G/4.97G [00:09<00:12, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/4.97G [00:09<00:14, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.97G [00:09<00:13, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/4.97G [00:09<00:13, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.97G [00:10<00:18, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 2.16G/4.97G [00:11<00:43, 65.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.97G [00:12<01:05, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 2.20G/4.97G [00:12<00:53, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.97G [00:12<00:37, 72.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.97G [00:12<00:25, 105MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.97G [00:13<00:28, 91.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.97G [00:13<00:22, 114MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 2.36G/4.97G [00:13<00:22, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.97G [00:13<00:20, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/4.97G [00:13<00:19, 132MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.97G [00:13<00:13, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.97G [00:13<00:11, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.97G [00:13<00:10, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 2.55G/4.97G [00:14<00:09, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.97G [00:14<00:08, 285MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.97G [00:14<00:12, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.97G [00:14<00:12, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.97G [00:14<00:11, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.97G [00:15<00:10, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 2.76G/4.97G [00:15<00:11, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 2.80G/4.97G [00:15<00:09, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/4.97G [00:15<00:08, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 2.86G/4.97G [00:15<00:09, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 2.90G/4.97G [00:15<00:08, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.97G [00:16<00:12, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [00:16<00:09, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 3.01G/4.97G [00:16<00:08, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.97G [00:16<00:08, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.97G [00:16<00:07, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 3.10G/4.97G [00:16<00:07, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.97G [00:16<00:06, 282MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.97G [00:16<00:06, 278MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.97G [00:17<00:06, 273MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.97G [00:17<00:06, 270MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.97G [00:17<00:07, 219MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 3.31G/4.97G [00:17<00:06, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 3.36G/4.97G [00:17<00:06, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [00:17<00:06, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.97G [00:17<00:06, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 3.46G/4.97G [00:18<00:05, 285MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.97G [00:18<00:05, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.97G [00:18<00:05, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 3.55G/4.97G [00:18<00:06, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.97G [00:18<00:06, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.97G [00:19<00:08, 163MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.97G [00:19<00:06, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.97G [00:19<00:06, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 3.71G/4.97G [00:19<00:07, 175MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.97G [00:19<00:06, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.97G [00:19<00:05, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.97G [00:19<00:04, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 3.86G/4.97G [00:19<00:03, 288MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/4.97G [00:20<00:06, 166MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.97G [00:20<00:06, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 3.95G/4.97G [00:20<00:05, 174MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/4.97G [00:20<00:04, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.97G [00:20<00:03, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.97G [00:21<00:03, 266MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 4.10G/4.97G [00:21<00:04, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/4.97G [00:21<00:03, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 4.16G/4.97G [00:21<00:04, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/4.97G [00:21<00:04, 166MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.97G [00:21<00:03, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.97G [00:22<00:03, 222MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 4.30G/4.97G [00:22<00:03, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.97G [00:22<00:03, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.97G [00:22<00:02, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.97G [00:22<00:02, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.97G [00:22<00:02, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.97G [00:22<00:01, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 4.50G/4.97G [00:23<00:02, 178MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 4.53G/4.97G [00:23<00:02, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 4.56G/4.97G [00:23<00:02, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.97G [00:23<00:02, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.97G [00:23<00:01, 177MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.97G [00:24<00:01, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/4.97G [00:24<00:01, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.97G [00:24<00:01, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 4.75G/4.97G [00:24<00:01, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.97G [00:25<00:01, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 4.80G/4.97G [00:25<00:00, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 4.82G/4.97G [00:25<00:01, 87.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 4.84G/4.97G [00:25<00:01, 101MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/4.97G [00:25<00:00, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 4.90G/4.97G [00:26<00:00, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.97G [00:26<00:00, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 4.94G/4.97G [00:26<00:00, 133MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [00:26<00:00, 186MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:26<00:00, 13.47s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "generation_config.json: 100% 189/189 [00:00<00:00, 1.35MB/s]\n",
            "Extracting Transformer layers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
            "Extracting: 100% 129/129 [00:35<00:00,  3.64it/s]\n",
            "Saved emotion activations to data/processed/anger/hidden_states/train_emotion_layers.npy, shape (28, 1025, 3072)\n",
            "Extracting: 100% 1603/1603 [26:25<00:00,  1.01it/s]\n",
            "Saved neutral activations to data/processed/anger/hidden_states/train_neutral_layers.npy, shape (28, 12823, 3072)\n"
          ]
        }
      ],
      "source": [
        "!python src/recorder.py --emotion anger --split train --model_name meta-llama/Llama-3.2-3B-Instruct --layers all --batch_size 8 --device cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZeWdgK16GyN",
        "outputId": "4188add4-a260-405a-f6a6-e0da60684c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded emotion data (28, 1025, 3072), neutral (28, 12823, 3072)\n",
            "Saved sorted heatmap to results/neuron_stats/anger/train_global/train_sorted_mean_diff_heatmap.png\n",
            "Saved scatter plot to results/neuron_stats/anger/train_global/train_top20_scatter.png\n",
            "Saved global ranking CSV to results/neuron_stats/anger/train_global/train_global_rank.csv\n"
          ]
        }
      ],
      "source": [
        "!python src/analyzer.py \\\n",
        "  --emotion anger \\\n",
        "  --split train \\\n",
        "  --emotion_npy data/processed/anger/hidden_states/train_emotion_layers.npy \\\n",
        "  --neutral_npy data/processed/anger/hidden_states/train_neutral_layers.npy \\\n",
        "  --top_k 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhARPQzd7oTh",
        "outputId": "78a688d7-1dd8-4474-af7f-5b073e720354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved diff matrix to data/processed/anger/steering_vector/train_top_k_diffmatrix.npy\n",
            "Matrix shape: (28, 3072), non-zero elements: 560\n"
          ]
        }
      ],
      "source": [
        "!python src/injector.py \\\n",
        "  --emotion anger \\\n",
        "  --split train \\\n",
        "  --vector_type top_k \\\n",
        "  --top_k 20 \\\n",
        "  --alpha 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lnAP6uYm873Y",
        "outputId": "aa0559fd-cefe-4d2b-f39e-257f3b26b00a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-01 14:43:03.489383: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-01 14:43:03.509765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746110583.531364    8719 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746110583.537898    8719 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-01 14:43:03.559802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.06s/it]\n",
            "Layer  0 cosine similarity: 1.0000\n",
            "Layer  1 cosine similarity: 0.9386\n",
            "Layer  2 cosine similarity: 0.8346\n",
            "Layer  3 cosine similarity: 0.7140\n",
            "Layer  4 cosine similarity: 0.7007\n",
            "Layer  5 cosine similarity: 0.7036\n",
            "Layer  6 cosine similarity: 0.7121\n",
            "Layer  7 cosine similarity: 0.6890\n",
            "Layer  8 cosine similarity: 0.7157\n",
            "Layer  9 cosine similarity: 0.7452\n",
            "Layer 10 cosine similarity: 0.7342\n",
            "Layer 11 cosine similarity: 0.6727\n",
            "Layer 12 cosine similarity: 0.6940\n",
            "Layer 13 cosine similarity: 0.6438\n",
            "Layer 14 cosine similarity: 0.6284\n",
            "Layer 15 cosine similarity: 0.6464\n",
            "Layer 16 cosine similarity: 0.6768\n",
            "Layer 17 cosine similarity: 0.6875\n",
            "Layer 18 cosine similarity: 0.6805\n",
            "Layer 19 cosine similarity: 0.6911\n",
            "Layer 20 cosine similarity: 0.6564\n",
            "Layer 21 cosine similarity: 0.6297\n",
            "Layer 22 cosine similarity: 0.6380\n",
            "Layer 23 cosine similarity: 0.6532\n",
            "Layer 24 cosine similarity: 0.6541\n",
            "Layer 25 cosine similarity: 0.5863\n",
            "Layer 26 cosine similarity: 0.4886\n",
            "Layer 27 cosine similarity: 0.2714\n",
            "Figure(1000x400)\n"
          ]
        }
      ],
      "source": [
        "!python src/hook_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LlBvc0W7tiw",
        "outputId": "661aee29-34b6-4665-ab9e-8dc574c79a62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-01 14:45:29.225712: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-01 14:45:29.246177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746110729.268284    9653 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746110729.274880    9653 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-01 14:45:29.297066: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "--- Baseline ---\n",
            "Tell me about yourself\n",
            "I'm a curious and creative person who loves to learn and explore new things. I enjoy trying new foods, traveling, and meeting new people. I'm also a bit of a bookworm and love getting lost in a good novel.\n",
            "\n",
            "I'm not sure what I want\n",
            "\n",
            "+++ Steered (α=0.8, layers=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]) +++\n",
            "Tell me about yourself.\n",
            "I'm a guy, 28, living in LA. I'm a wannabe actor, wannabe musician, wannabe... you know, wannabe somebody. I work at a video game store, selling games to idiots who think they're gonna be a pro gamer\n",
            "\n",
            "Results saved to results.json\n"
          ]
        }
      ],
      "source": [
        "!python src/evaluator.py \\\n",
        "  --model_name meta-llama/Llama-3.2-3B-Instruct \\\n",
        "  --vector_path data/processed/anger/steering_vector/train_top_k_diffmatrix.npy \\\n",
        "  --layers all \\\n",
        "  --alpha 0.8 \\\n",
        "  --do_sample \\\n",
        "  --temperature 0.7 \\\n",
        "  --top_p 0.9 \\\n",
        "  --seed 123 \\\n",
        "  --output_file results.json \\\n",
        "  \"Tell me about yourself\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEX2spvL8OLr",
        "outputId": "58490e93-1d65-4a87-8e3e-efe3af48378b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-01 14:43:43.289644: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-01 14:43:43.306339: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746110623.328172    9009 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746110623.334839    9009 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-01 14:43:43.355551: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 1.00k/1.00k [00:00<00:00, 6.39MB/s]\n",
            "pytorch_model.bin: 100% 329M/329M [00:04<00:00, 69.5MB/s]\n",
            "tokenizer_config.json: 100% 294/294 [00:00<00:00, 1.91MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 1.25MB/s]\n",
            "merges.txt:   0% 0.00/456k [00:00<?, ?B/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.06MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 1.59MB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.93MB/s]\n",
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "\n",
            "Loading checkpoint shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model.safetensors:  59% 194M/329M [00:04<00:02, 53.9MB/s]\u001b[A\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.01it/s]\n",
            "\n",
            "model.safetensors: 100% 329M/329M [00:06<00:00, 48.9MB/s]\n",
            "\n",
            "=== Prompt ===\n",
            "Tell me about yourself\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "\n",
            "--- Baseline:\n",
            "Tell me about yourself\n",
            "I'm a 25-year-old woman from a small town in the Midwest. I grew up in a close-knit community where everyone knows each other, and I was always the type of person who was a bit of a people person. I love meeting new people and\n",
            "Anger prob: 0.0049\n",
            "\n",
            "+++ Steered (layers=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]):\n",
            "Tell me about yourself\n",
            "I'm a 25-year-old, 5'10\", 180-pound, white, male, from... (wait, no, I'm not gonna say that, I'ma say I'ma say I'ma say I'ma say I'm\n",
            "Anger prob: 0.0244\n",
            "\n",
            "Avg anger prob: baseline=0.0049, steered=0.0244, Δ=0.0195\n",
            "Results saved to results.json\n"
          ]
        }
      ],
      "source": [
        "!python src/evaluator_classification.py \\\n",
        "  --model_name meta-llama/Llama-3.2-3B-Instruct \\\n",
        "  --vector_path data/processed/anger/steering_vector/train_top_k_diffmatrix.npy \\\n",
        "  --layers all \\\n",
        "  --emotion anger \\\n",
        "  --device cuda \\\n",
        "  --max_length 60 \\\n",
        "  --seed 42 \\\n",
        "  --output_file results.json \\\n",
        "  \"Tell me about yourself\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXiobMvz8p8f",
        "outputId": "fcaf883f-c11d-4560-be4c-6aeb4421a882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "README.md: 100% 9.59k/9.59k [00:00<00:00, 43.3MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 223k/223k [00:00<00:00, 76.1MB/s]\n",
            "Generating validation split: 100% 817/817 [00:00<00:00, 96351.65 examples/s]\n",
            "Wrote 817 examples to data/benchmarks/truthfulqa_validation.json\n"
          ]
        }
      ],
      "source": [
        "!python src/data_loader_truthful.py \\\n",
        "  --splits validation \\\n",
        "  --output_dir data/benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4IaD4FA9nSm",
        "outputId": "54a5432f-d312-4df0-dc06-ff9e74c14da7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-01 14:46:24.745782: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-01 14:46:24.766155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746110784.788035    9974 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746110784.794541    9974 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-01 14:46:24.816724: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "Evaluating TruthfulQA:   0% 0/817 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Evaluating TruthfulQA:   2% 20/817 [06:23<4:13:06, 19.05s/it]"
          ]
        }
      ],
      "source": [
        "!python src/evaluator_benchmark.py \\\n",
        "  --model_name meta-llama/Llama-3.2-3B-Instruct \\\n",
        "  --vector_path data/processed/anger/steering_vector/train_top_k_diffmatrix.npy \\\n",
        "  --layers all \\\n",
        "  --alpha 1.0 \\\n",
        "  --device cuda \\\n",
        "  --max_length 200 \\\n",
        "  --seed 42 \\\n",
        "  --data_path data/benchmarks/truthfulqa_validation.json \\\n",
        "  --output_file tq_results.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2UbQ6oH-4TB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
